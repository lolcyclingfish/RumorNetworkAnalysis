{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classifiers\n",
    "\n",
    "This notebook to classify the probability of fake news traveling from A to B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mClassificationModelInput.csv\u001b[m\u001b[m* \u001b[31mpol_agg_new.csv\u001b[m\u001b[m*\r\n",
      "\u001b[31memergent.csv\u001b[m\u001b[m*                 \u001b[31mpolitifact.csv\u001b[m\u001b[m*\r\n",
      "\u001b[31mfake_localcentralities.csv\u001b[m\u001b[m*   \u001b[31mpolitifact_clean.csv\u001b[m\u001b[m*\r\n",
      "\u001b[31mkey_mutuality_roshan.csv\u001b[m\u001b[m*     \u001b[31mreal_localcentralities.csv\u001b[m\u001b[m*\r\n",
      "\u001b[31mkeys.csv\u001b[m\u001b[m*                     \u001b[31msnopes.csv\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd, pyprind\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score,make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Source', u'Destination', u'Monday', u'Elections', u'Military',\n",
      "       u'Thursday', u'Religion', u'Friday', u'Saturday', u'TRUE', u'History',\n",
      "       u'Tuesday', u'Wednesday', u'Sunday', u'page_url', u'Health Care',\n",
      "       u'FALSE', u'jaccard_coeff_connection', u'Neighbor_connection',\n",
      "       u'jaccard_coeff_common_destination', u'Neighbor_common_destination',\n",
      "       u'jaccard_coeff_common_source', u'Neighbor_common_source',\n",
      "       u'SourceSite_outdeg_real', u'DestSite_outdeg_real',\n",
      "       u'SourceSite_outdeg_fake', u'DestSite_outdeg_fake',\n",
      "       u'SourceSite_indeg_fake', u'DestSite_indeg_fake',\n",
      "       u'DestSite_indeg_real', u'SourceSite_indeg_real', u'mutuality_ind',\n",
      "       u'Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "d=pd.read_csv('data/ClassificationModelInput.csv')\n",
    "sum(sum(d.isnull())) #No NA\n",
    "print(d.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 33)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "d = d[d['Source'] == 'www.facebook.com']\n",
    "#d = d[d['Destination'] == 'www.politifact.com']\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=['Monday', 'Elections', 'Military', 'Thursday',\n",
    "       'Religion', 'Friday', 'Saturday','History', 'Tuesday',\n",
    "       'Wednesday', 'Sunday', 'page_url', 'Health Care',\n",
    "       'jaccard_coeff_connection', 'Neighbor_connection',\n",
    "       'jaccard_coeff_common_destination', 'Neighbor_common_destination',\n",
    "       'jaccard_coeff_common_source', 'Neighbor_common_source',\n",
    "       'SourceSite_outdeg_fake', 'DestSite_outdeg_fake',\n",
    "       'SourceSite_indeg_fake', 'DestSite_indeg_fake', 'mutuality_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2.175901\n",
       "3    -0.513445\n",
       "6     1.548472\n",
       "9     0.676533\n",
       "12    0.676533\n",
       "Name: Monday, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in ['Monday', 'Tuesday', 'Wednesday', \n",
    "                            'Thursday','Friday', 'Saturday', 'Sunday']:\n",
    "    mean = d[col].mean()\n",
    "    std = d[col].std()\n",
    "    if std != 0:\n",
    "        d[col] = d[col].apply(lambda row: (row - mean)/std).apply(lambda row: math.log(row+1))\n",
    "    else:\n",
    "        d[col] = 0\n",
    "d['Label']=d['FALSE'].apply(lambda x: int(x>0))\n",
    "#d['Label'] = d['FAKE']/(d['TRUE'] + d['FAKE'])\n",
    "#Check for class imblanace\n",
    "#print(float(sum(d['Label']==0))/len(d))\n",
    "\n",
    "#Ready for model\n",
    "X=d[features]\n",
    "Y=d['Label']\n",
    "d['Monday'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Random Forest, GBM\n",
    "\n",
    "#Make AUC the performance metric\n",
    "models = {'RF':{'model':RandomForestClassifier(),\n",
    "                     'Params':{'n_estimators':range(50,70,10),\n",
    "                              'max_depth':range(2,5), 'class_weight':['balanced']}},\n",
    "                \n",
    "         'GBM':{'model':GradientBoostingClassifier(),\n",
    "                     'Params':{'n_estimators':range(50,100,10),\n",
    "                               'max_depth':range(2,5),\n",
    "                               'learning_rate':linspace(0.1,0.5,num=20)}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of features:', 24)\n",
      "('Number of records', 167)\n"
     ]
    }
   ],
   "source": [
    "print('Number of features:', len(X.columns))\n",
    "print('Number of records', len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modeleva(cvfolds):\n",
    "    allmodels={}\n",
    "    for model in models.keys():\n",
    "        grid = GridSearchCV(estimator=models[model]['model'], \n",
    "                            param_grid=models[model]['Params'],\n",
    "                            cv=cvfolds, n_jobs=-1,scoring='roc_auc')\n",
    "        modelstats={}\n",
    "        modelstats['model']=grid.fit(X, Y)\n",
    "        modelstats['best_estimator']=modelstats['model'].best_estimator_\n",
    "        modelstats['best_score']=modelstats['model'].best_score_\n",
    "        allmodels[model]=modelstats\n",
    "        del modelstats\n",
    "        print(model)\n",
    "    results=pd.DataFrame.from_dict(allmodels).T.sort_values(by='best_score')\n",
    "    results=results.reset_index()\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF\n",
      "GBM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>best_estimator</th>\n",
       "      <th>best_score</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.869171</td>\n",
       "      <td>GridSearchCV(cv=3, error_score='raise',\\n     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GBM</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>0.915578</td>\n",
       "      <td>GridSearchCV(cv=3, error_score='raise',\\n     ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                     best_estimator best_score  \\\n",
       "0    RF  (DecisionTreeClassifier(class_weight=None, cri...   0.869171   \n",
       "1   GBM  ([DecisionTreeRegressor(criterion='friedman_ms...   0.915578   \n",
       "\n",
       "                                               model  \n",
       "0  GridSearchCV(cv=3, error_score='raise',\\n     ...  \n",
       "1  GridSearchCV(cv=3, error_score='raise',\\n     ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allresults=modeleva(3)\n",
    "allresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=2, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allresults['best_estimator'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bestmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fd16a56f59a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbestmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bestmodel' is not defined"
     ]
    }
   ],
   "source": [
    "bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: unicode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-1761114a045a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'you are a fucking asshole'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unicode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m: unknown encoding: unicode"
     ]
    }
   ],
   "source": [
    "k='you are a fucking asshole'.decode('unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find the sentiment analyzer you want to load.\n",
      "Training a new one using NaiveBayesClassifier.\n",
      "Training classifier\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-26d7ab3eb482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdemo_subjectivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdemo_sent_subjectivity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'you are a fucking asshole'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/chaoranwei/anaconda/lib/python2.7/site-packages/nltk/sentiment/util.py\u001b[0m in \u001b[0;36mdemo_sent_subjectivity\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot find the sentiment analyzer you want to load.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training a new one using NaiveBayesClassifier.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0msentim_analyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo_subjectivity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;31m# Tokenize and convert to lower case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoranwei/anaconda/lib/python2.7/site-packages/nltk/sentiment/util.py\u001b[0m in \u001b[0;36mdemo_subjectivity\u001b[0;34m(trainer, save_analyzer, n_instances, output)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentim_analyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentim_analyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoranwei/anaconda/lib/python2.7/site-packages/nltk/sentiment/sentiment_analyzer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, training_set, save_classifier, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_classifier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0msave_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoranwei/anaconda/lib/python2.7/site-packages/nltk/classify/naivebayes.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Count up how many times each feature value occurred, given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# the label and featurename.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mlabel_freqdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoranwei/anaconda/lib/python2.7/site-packages/nltk/util.pyc\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lists\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoranwei/anaconda/lib/python2.7/site-packages/nltk/classify/util.pyc\u001b[0m in \u001b[0;36mlazy_func\u001b[0;34m(labeled_token)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlazy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeature_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLazyMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoranwei/anaconda/lib/python2.7/site-packages/nltk/sentiment/sentiment_analyzer.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mextractor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_extractors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_extractors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparam_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mall_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoranwei/anaconda/lib/python2.7/site-packages/nltk/sentiment/util.py\u001b[0m in \u001b[0;36mextract_unigram_feats\u001b[0;34m(document, unigrams, handle_negation)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmark_negation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contains({0})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.util import demo_subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer \n",
    "demo_sent_subjectivity('you are a fucking asshole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import demo_SENT_subjectivity\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
